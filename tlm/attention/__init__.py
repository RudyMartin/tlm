# tlm.attention - Pure Python attention mechanisms for transformers